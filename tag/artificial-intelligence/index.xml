<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Artificial Intelligence | Annibale Panichella</title>
    <link>https://example.com/tag/artificial-intelligence/</link>
      <atom:link href="https://example.com/tag/artificial-intelligence/index.xml" rel="self" type="application/rss+xml" />
    <description>Artificial Intelligence</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 Jan 2024 14:29:50 +0200</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Artificial Intelligence</title>
      <link>https://example.com/tag/artificial-intelligence/</link>
    </image>
    
    <item>
      <title>Breaking the Silence: the Threats of Using LLMs in Software Engineering</title>
      <link>https://example.com/publication/icse-nier2024/</link>
      <pubDate>Mon, 01 Jan 2024 14:29:50 +0200</pubDate>
      <guid>https://example.com/publication/icse-nier2024/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Large Language Models (LLMs) have gained considerable traction within the Software Engineering (SE) community, impacting various SE tasks from code completion to test generation, from program repair to code summarization. Despite their promise, researchers must still be careful as numerous intricate factors can influence the outcomes of experiments involving LLMs.
This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings.
In response, this paper proposes a set of guidelines tailored for SE researchers and Language Model (LM) providers to mitigate these concerns.
The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Oracle Issues in Machine Learning and Where to Find Them</title>
      <link>https://example.com/publication/raise2020/</link>
      <pubDate>Tue, 03 Mar 2020 16:20:15 +0100</pubDate>
      <guid>https://example.com/publication/raise2020/</guid>
      <description>&lt;p&gt;Abstract:
The rise in popularity of machine learning (ML), and deep learning in particular, has both led to optimism about achievements of artificial intelligence, as well as concerns about possible weaknesses and vulnerabilities of ML pipelines. Within the software engineering community, this has led to a considerable body of work on ML testing techniques, including white- and black-box testing for ML models. This means the oracle problem needs to be addressed; for supervised ML applications, oracle information is indeed available in the form of dataset &amp;ldquo;ground truth&amp;rdquo;, that encodes input data with corresponding desired output labels. However, while ground truth forms a gold standard, there still is no guarantee it is truly correct. Indeed, syntactic, semantic, and conceptual framing issues in the oracle may negatively affect the ML system integrity. While syntactic issues may be automatically verified and corrected, the higher-level issues traditionally need human judgment and manual analysis.
In this paper, we employ two heuristics based on information entropy and semantic analysis on well-known computer vision models and benchmark data from ImageNet. The heuristics are used to semi-automatically uncover potential higher-level issues in (i) the label taxonomy used to define the ground truth oracle (labels), and (ii) data encoding and representation. In doing this, beyond existing ML testing efforts, we illustrate the need for SE strategies that especially target and assess the oracle.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cyber-Physical Systems | Annibale Panichella</title>
    <link>https://example.com/tag/cyber-physical-systems/</link>
      <atom:link href="https://example.com/tag/cyber-physical-systems/index.xml" rel="self" type="application/rss+xml" />
    <description>Cyber-Physical Systems</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 09 Sep 2025 11:21:45 +0200</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Cyber-Physical Systems</title>
      <link>https://example.com/tag/cyber-physical-systems/</link>
    </image>
    
    <item>
      <title>Metamorphic Testing of Deep Code Models: A Systematic Literature Review</title>
      <link>https://example.com/publication/acm-tosem2025/</link>
      <pubDate>Tue, 09 Sep 2025 11:21:45 +0200</pubDate>
      <guid>https://example.com/publication/acm-tosem2025/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;
Large language models and deep learning models designed for code intelligence have revolutionized the software engineering field due to their ability to perform various code-related tasks. These models can process source code and software artifacts with high accuracy in tasks such as code completion, defect detection, and code summarization; therefore, they can potentially become an integral part of modern software engineering practices. Despite these capabilities, robustness remains a critical quality attribute for deep-code models as they may produce different results under varied and adversarial conditions (e.g., variable renaming). Metamorphic testing has become a widely used approach to evaluate modelsâ€™ robustness by applying semantic-preserving transformations to input programs and analyzing the stability of model outputs. While prior research has explored testing deep learning models, this systematic literature review focuses specifically on metamorphic testing for deep code models. By studying 45 primary papers, we analyze the transformations, techniques, and evaluation methods used to assess robustness. Our review summarizes the current landscape, identifying frequently evaluated models, programming tasks, datasets, target languages, and evaluation metrics, and highlights key challenges and future directions for advancing the field.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Danger is My Middle Lane: Simulations from Real-World Dangerous Roads</title>
      <link>https://example.com/publication/ssbse-nier2024/</link>
      <pubDate>Thu, 06 Jun 2024 09:16:23 +0200</pubDate>
      <guid>https://example.com/publication/ssbse-nier2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Slow and The Furious? Performance Antipattern Detection in Cyber-Physical Systems</title>
      <link>https://example.com/publication/jss2023/</link>
      <pubDate>Mon, 01 Jan 2024 17:32:54 +0200</pubDate>
      <guid>https://example.com/publication/jss2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Continuous Integration and Delivery practices for Cyber- Physical systems: An interview-based study</title>
      <link>https://example.com/publication/acm-tosem2022b/</link>
      <pubDate>Fri, 11 Nov 2022 11:21:45 +0200</pubDate>
      <guid>https://example.com/publication/acm-tosem2022b/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>

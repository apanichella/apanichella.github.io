<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Evolutionary Testing | Annibale Panichella</title>
    <link>https://example.com/tag/evolutionary-testing/</link>
      <atom:link href="https://example.com/tag/evolutionary-testing/index.xml" rel="self" type="application/rss+xml" />
    <description>Evolutionary Testing</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 Jan 2024 19:37:11 +0200</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Evolutionary Testing</title>
      <link>https://example.com/tag/evolutionary-testing/</link>
    </image>
    
    <item>
      <title>TestSpark: IntelliJ IDEAâ€™s Ultimate Test Generation Companion</title>
      <link>https://example.com/publication/icse2024-tool2/</link>
      <pubDate>Mon, 01 Jan 2024 19:37:11 +0200</pubDate>
      <guid>https://example.com/publication/icse2024-tool2/</guid>
      <description>&lt;p&gt;Abstract:&lt;/p&gt;
&lt;p&gt;Writing software tests is laborious and time-consuming. To address
this, prior studies introduced various automated test-generation
techniques. A well-explored research direction in this field is unit
test generation, wherein artificial intelligence (AI) techniques create
tests for a method/class under test. While many of these techniques
have primarily found applications in a research context, existing
tools (e.g., EvoSuite, Randoop, and AthenaTest) are not user-friendly
and are tailored to a single technique. This paper introduces Test-
Spark, a plugin for IntelliJ IDEA that enables users to generate unit
tests with only a few clicks directly within their Integrated De-
velopment Environment (IDE). Furthermore, TestSpark also allows
users to easily modify and run each generated test and integrate
them into the project workflow. TestSpark leverages the advances of
search-based test generation tools, and it introduces a technique to
generate unit tests using Large Language Models (LLMs) by creating
a feedback cycle between the IDE and the LLM. Since TestSpark is
an open-source (&lt;a href=&#34;https://github.com/JetBrains-Research/TestSpark%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/JetBrains-Research/TestSpark)&lt;/a&gt;,
extendable, and well-documented tool, it is possible to add new test
generation methods into the plugin with the minimum effort. This
paper also explains our future studies related to TestSpark and our
preliminary results.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Testing with Fewer Resources: An Adaptive Approach to Performance-Aware Test Case Generation</title>
      <link>https://example.com/publication/ieee-tse2019/</link>
      <pubDate>Wed, 09 Oct 2019 09:51:33 +0200</pubDate>
      <guid>https://example.com/publication/ieee-tse2019/</guid>
      <description>&lt;p&gt;Abstract: Automated test case generation is an effective technique to yield high-coverage test suites.
While the majority of research effort has been devoted to satisfying coverage criteria, a recent trend emerged towards optimizing other non-coverage aspects.
In this regard, runtime and memory usage are two essential dimensions: less expensive tests reduce the resource demands for the generation process and later regression testing phases.
This study shows that performance-aware test case generation requires solving two main challenges:
providing a good approximation of resource usage with minimal overhead and&lt;br&gt;
avoiding detrimental effects on both final coverage and fault detection effectiveness.
To tackle these challenges, we conceived a set of performance proxies -inspired by previous work on performance testing- that provide a reasonable estimation of the test execution costs (i.e., runtime and memory usage).
Thus, we propose an adaptive strategy, called aDynaMOSA, which leverages these proxies by extending DynaMOSA, a state-of-the-art evolutionary algorithm in unit testing.
Our empirical study -involving 110 non-trivial Java classes- reveals
that our adaptive approach generates test suite with statistically significant improvements in runtime (-25%) and heap memory consumption (-15%) compared to DynaMOSA. Additionally, aDynaMOSA has comparable results to DynaMOSA over seven different coverage criteria and similar fault detection effectiveness.
Our empirical investigation also highlights that the usage of performance proxies (i.e., without the adaptiveness) is not sufficient to generate more performant test cases without compromising the overall coverage.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automated Test Case Generation as a Many-Objective Optimisation Problem with Dynamic Selection of the Targets</title>
      <link>https://example.com/publication/ieee-tse2018b/</link>
      <pubDate>Tue, 10 Jul 2018 11:23:34 +0200</pubDate>
      <guid>https://example.com/publication/ieee-tse2018b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evolutionary Testing for Crash Reproduction</title>
      <link>https://example.com/publication/sbst2016a/</link>
      <pubDate>Thu, 18 Aug 2016 07:56:19 +0200</pubDate>
      <guid>https://example.com/publication/sbst2016a/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Large Language Models | Annibale Panichella</title>
    <link>https://example.com/tag/large-language-models/</link>
      <atom:link href="https://example.com/tag/large-language-models/index.xml" rel="self" type="application/rss+xml" />
    <description>Large Language Models</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 19 Jan 2025 09:32:39 +0100</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Large Language Models</title>
      <link>https://example.com/tag/large-language-models/</link>
    </image>
    
    <item>
      <title>Metamorphic-Based Many-Objective Distillation of LLMs for Code-related Tasks</title>
      <link>https://example.com/publication/icse2025/</link>
      <pubDate>Sun, 19 Jan 2025 09:32:39 +0100</pubDate>
      <guid>https://example.com/publication/icse2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation</title>
      <link>https://example.com/publication/icst2025a/</link>
      <pubDate>Wed, 01 Jan 2025 00:03:02 +0200</pubDate>
      <guid>https://example.com/publication/icst2025a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TestSpark: IntelliJ IDEAâ€™s Ultimate Test Generation Companion</title>
      <link>https://example.com/publication/icse2024-tool2/</link>
      <pubDate>Mon, 01 Jan 2024 19:37:11 +0200</pubDate>
      <guid>https://example.com/publication/icse2024-tool2/</guid>
      <description>&lt;p&gt;Abstract:&lt;/p&gt;
&lt;p&gt;Writing software tests is laborious and time-consuming. To address
this, prior studies introduced various automated test-generation
techniques. A well-explored research direction in this field is unit
test generation, wherein artificial intelligence (AI) techniques create
tests for a method/class under test. While many of these techniques
have primarily found applications in a research context, existing
tools (e.g., EvoSuite, Randoop, and AthenaTest) are not user-friendly
and are tailored to a single technique. This paper introduces Test-
Spark, a plugin for IntelliJ IDEA that enables users to generate unit
tests with only a few clicks directly within their Integrated De-
velopment Environment (IDE). Furthermore, TestSpark also allows
users to easily modify and run each generated test and integrate
them into the project workflow. TestSpark leverages the advances of
search-based test generation tools, and it introduces a technique to
generate unit tests using Large Language Models (LLMs) by creating
a feedback cycle between the IDE and the LLM. Since TestSpark is
an open-source (&lt;a href=&#34;https://github.com/JetBrains-Research/TestSpark%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/JetBrains-Research/TestSpark)&lt;/a&gt;,
extendable, and well-documented tool, it is possible to add new test
generation methods into the plugin with the minimum effort. This
paper also explains our future studies related to TestSpark and our
preliminary results.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Breaking the Silence: the Threats of Using LLMs in Software Engineering</title>
      <link>https://example.com/publication/icse-nier2024/</link>
      <pubDate>Mon, 01 Jan 2024 14:29:50 +0200</pubDate>
      <guid>https://example.com/publication/icse-nier2024/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Large Language Models (LLMs) have gained considerable traction within the Software Engineering (SE) community, impacting various SE tasks from code completion to test generation, from program repair to code summarization. Despite their promise, researchers must still be careful as numerous intricate factors can influence the outcomes of experiments involving LLMs.
This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings.
In response, this paper proposes a set of guidelines tailored for SE researchers and Language Model (LM) providers to mitigate these concerns.
The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

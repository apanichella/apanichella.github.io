<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>search-based software engineering | Annibale Panichella</title>
    <link>/tag/search-based-software-engineering/</link>
      <atom:link href="/tag/search-based-software-engineering/index.xml" rel="self" type="application/rss+xml" />
    <description>search-based software engineering</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 06 Jul 2020 14:29:50 +0200</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>search-based software engineering</title>
      <link>/tag/search-based-software-engineering/</link>
    </image>
    
    <item>
      <title>Generating Highly-structured Input Data by Combining Search-based Testing and Grammar-based Fuzzing</title>
      <link>/publication/ase2020-nier/</link>
      <pubDate>Mon, 06 Jul 2020 14:29:50 +0200</pubDate>
      <guid>/publication/ase2020-nier/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Botsing, a Search-based Crash Reproduction Framework for Java</title>
      <link>/publication/ase2020-tool/</link>
      <pubDate>Sun, 05 Jul 2020 14:19:53 +0200</pubDate>
      <guid>/publication/ase2020-tool/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automated Repair of Feature Interaction Failures in Automated Driving Systems</title>
      <link>/publication/issta2020/</link>
      <pubDate>Fri, 17 Apr 2020 17:32:54 +0200</pubDate>
      <guid>/publication/issta2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automated Test Generation for Unit Testing Beyond</title>
      <link>/talk/ipa2019/</link>
      <pubDate>Mon, 28 Oct 2019 11:17:15 +0200</pubDate>
      <guid>/talk/ipa2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Testing with Fewer Resources: An Adaptive Approach to Performance-Aware Test Case Generation</title>
      <link>/publication/ieee-tse2019/</link>
      <pubDate>Wed, 09 Oct 2019 09:51:33 +0200</pubDate>
      <guid>/publication/ieee-tse2019/</guid>
      <description>&lt;p&gt;Abstract: Automated test case generation is an effective technique to yield high-coverage test suites.
While the majority of research effort has been devoted to satisfying coverage criteria, a recent trend emerged towards optimizing other non-coverage aspects.
In this regard, runtime and memory usage are two essential dimensions: less expensive tests reduce the resource demands for the generation process and later regression testing phases.
This study shows that performance-aware test case generation requires solving two main challenges:
providing a good approximation of resource usage with minimal overhead and&lt;br&gt;
avoiding detrimental effects on both final coverage and fault detection effectiveness.
To tackle these challenges, we conceived a set of performance proxies -inspired by previous work on performance testing- that provide a reasonable estimation of the test execution costs (i.e., runtime and memory usage).
Thus, we propose an adaptive strategy, called aDynaMOSA, which leverages these proxies by extending DynaMOSA, a state-of-the-art evolutionary algorithm in unit testing.
Our empirical study -involving 110 non-trivial Java classes- reveals
that our adaptive approach generates test suite with statistically significant improvements in runtime (-25%) and heap memory consumption (-15%) compared to DynaMOSA. Additionally, aDynaMOSA has comparable results to DynaMOSA over seven different coverage criteria and similar fault detection effectiveness.
Our empirical investigation also highlights that the usage of performance proxies (i.e., without the adaptiveness) is not sufficient to generate more performant test cases without compromising the overall coverage.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Search-Based-LDA</title>
      <link>/tools/ssbse-lda/</link>
      <pubDate>Wed, 26 Jun 2019 10:29:32 +0200</pubDate>
      <guid>/tools/ssbse-lda/</guid>
      <description>&lt;p&gt;A Systematic Comparison of Search Algorithms for Topic Modelling - A Study on DuplicateBug Report Identification&lt;/p&gt;
&lt;h3 id=&#34;author&#34;&gt;Author&lt;/h3&gt;
&lt;p&gt;Annibale Panichella&lt;/p&gt;
&lt;h3 id=&#34;venue&#34;&gt;Venue&lt;/h3&gt;
&lt;p&gt;SSBSE 2019&lt;/p&gt;
&lt;h4 id=&#34;abstract&#34;&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Latent Dirichlet Allocation (LDA) has been used to supportmany software engineering tasks. Previous studies showed that defaultsettings lead to sub-optimal topic modeling with a dramatic impacton the performance of such approaches in terms of precision andrecall. For this reason, researchers used search algorithms (e.g., geneticalgorithms) to automatically configure topic models in an unsupervisedfashion. While previous work showed the ability of individual search algorithmsin finding near-optimal configurations, it is not clear to whatextent the choice of the meta-heuristic matters for SE tasks. In this paper,we present a systematic comparison of five different meta-heuristicsto configure LDA in the context of duplicate bug reports identification.The results show that (1) no master algorithm outperforms the othersfor all software projects, (2) random search and PSO are the least effectivemeta-heuristics. Finally, the running time strongly depends on thecomputational complexity of LDA while the internal complexity of thesearch algorithms plays a negligible role.&lt;/p&gt;
&lt;h3 id=&#34;getting-started&#34;&gt;Getting started&lt;/h3&gt;
&lt;p&gt;The source code is available in GitHub at the link: &lt;a href=&#34;https://github.com/apanichella/Search-Based-LDA&#34;&gt;https://github.com/apanichella/Search-Based-LDA&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Effective and Efficient API Misuse Detection via Exception Propagation and Search-based Testing</title>
      <link>/publication/issta2019/</link>
      <pubDate>Wed, 01 May 2019 20:22:51 +0200</pubDate>
      <guid>/publication/issta2019/</guid>
      <description>&lt;p&gt;&lt;b&gt; Abstract &lt;/b&gt;: Application Programming Interfaces (APIs)
typically come with (implicit) usage constraints.
The violations of these constraints (API misuses)
can lead to software crashes.
Even though there are several tools that
can detect API misuses,
most of them suffer from a very high rate of false positives.
We introduce Catcher, a novel API misuse detection approach
that combines static exception propagation analysis with automatic search-based test case
generation to effectively and efficiently pinpoint crash-prone API misuses
in client applications.
We validate Catcher against 21 Java applications,
targeting misuses of the Java platform&amp;rsquo;s API.
Our results indicate that Catcher is able to generate
test cases that uncover 243 (unique) API misuses that result in
crashes.
Our empirical evaluation shows that Catcher can detect a large number of misuses (77 cases)
that would remain undetected by the traditional coverage-based test case generator EvoSuite.
Additionally, Catcher is on average eight times faster than EvoSuite
in generating test cases for the identified misuses.
Finally, we find that the majority of the exceptions triggered by Catcher
are unexpected to developers i.e., not only unhandled in the source code but also not listed in the documentation of the client applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Systematic Comparison of Search Algorithms for Topic Modelling - A Study on Duplicate Bug Report Identification</title>
      <link>/publication/ssbse2019/</link>
      <pubDate>Mon, 29 Apr 2019 23:16:23 +0200</pubDate>
      <guid>/publication/ssbse2019/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Latent Dirichlet Allocation (LDA) has been used to support many software engineering tasks. Previous studies showed that default settings lead to sub-optimal topic modeling with a dramatic impact on the performance of such approaches in terms of precision and recall. For this reason, researchers used search algorithms (e.g., genetic algorithms) to automatically configure topic models in an unsupervised fashion. While previous work showed the ability of individual search algorithms in finding near-optimal configurations, it is not clear to what extent the choice of the meta-heuristic matters for SE tasks. In this paper, we present a systematic comparison of five different meta-heuristics to configure LDA in the context of duplicate bug reports identification. The results show that (1) no master algorithm outperforms the others for all software projects, (2) random search and PSO are the least effective meta-heuristics. Finally, the running time strongly depends on the computational complexity of LDA while the internal complexity of the search algorithms plays a negligible role.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Incremental Control Dependency Frontier Exploration for Many-Criteria Test Case Generation</title>
      <link>/publication/ssbse2018b/</link>
      <pubDate>Fri, 20 Jul 2018 00:07:54 +0200</pubDate>
      <guid>/publication/ssbse2018b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Search-based Approach for Accurate Identification of Log Message Formats</title>
      <link>/publication/icpc2018/</link>
      <pubDate>Fri, 20 Jul 2018 00:05:13 +0200</pubDate>
      <guid>/publication/icpc2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Testing Autonomous Cars for Feature Interaction Failures using Many-Objective Search</title>
      <link>/publication/ase2018/</link>
      <pubDate>Fri, 20 Jul 2018 00:01:56 +0200</pubDate>
      <guid>/publication/ase2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automated Test Case Generation as a Many-Objective Optimisation Problem with Dynamic Selection of the Targets</title>
      <link>/publication/ieee-tse2018b/</link>
      <pubDate>Tue, 10 Jul 2018 11:23:34 +0200</pubDate>
      <guid>/publication/ieee-tse2018b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LIPS vs MOSA: a Replicated Empirical Study on Automated Test Case Generation</title>
      <link>/publication/ssbse2017/</link>
      <pubDate>Thu, 13 Jul 2017 20:03:02 +0200</pubDate>
      <guid>/publication/ssbse2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Improving Multi-Objective Search Based Test Suite Optimization through Diversity Injection</title>
      <link>/publication/tse2015/</link>
      <pubDate>Sun, 19 Jul 2015 23:46:36 +0200</pubDate>
      <guid>/publication/tse2015/</guid>
      <description>&lt;p&gt;Abstract:
A way to reduce the cost of regression testing consists of selecting or prioritizing subsets of test cases from a test suite according to some criteria. Besides greedy algorithms, cost cognizant additional greedy algorithms, multi-objective optimization algorithms, and multi-objective genetic algorithms (MOGAs), have also been proposed to tackle this problem. However, previous studies have shown that there is no clear winner between greedy and MOGAs, and that their combination does not necessarily produce better results. In this paper we show that the optimality of MOGAs can be significantly improved by diversifying the solutions (sub-sets of the test suite) generated during the search process. Specifically, we introduce a new MOGA, coined as DIversity based Genetic Algorithm (DIV-GA), based on the mechanisms of orthogonal design and orthogonal evolution that increase diversity by injecting new orthogonal individuals during the search process. Results of an empirical study conducted on eleven programs show that DIV-GA outperforms both greedy algorithms and the traditional MOGAs from the optimality point of view. Moreover, the solutions (sub-sets of the test suite) provided by DIV-GA are able to detect more faults than the other algorithms, while keeping the same test execution cost.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

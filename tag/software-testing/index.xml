<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>software testing | Annibale Panichella</title>
    <link>https://example.com/tag/software-testing/</link>
      <atom:link href="https://example.com/tag/software-testing/index.xml" rel="self" type="application/rss+xml" />
    <description>software testing</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 05 May 2025 14:29:50 +0200</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>software testing</title>
      <link>https://example.com/tag/software-testing/</link>
    </image>
    
    <item>
      <title>Automated Test-Case Generation for REST APIs Using Model Inference Search Heuristic</title>
      <link>https://example.com/publication/ast2025/</link>
      <pubDate>Mon, 05 May 2025 14:29:50 +0200</pubDate>
      <guid>https://example.com/publication/ast2025/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;The rising popularity of the microservice architectural style has led to a growing demand for automated testing approaches tailored to these systems. EvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to automatically generate test cases for microservices’ REST APIs. One limitation of these EAs is the use of unit-level search heuristics, such as branch distances, which focus on fine-grained code coverage and may not effectively capture the complex, interconnected behaviors characteristic of system-level testing. To address this limitation, we propose a new search heuristic (MISH) that uses real-time automaton learning to guide the test case generation process. We capture the sequential call patterns exhibited by a test case by learning an automaton from the stream of log events outputted by different microservices within the same system. Therefore, MISH learns a representation of the systemwide behavior, allowing us to define the fitness of a test case based on the path it traverses within the inferred automaton. We empirically evaluate MISH’s effectiveness on six real-world benchmark microservice applications and compare it against a state-of-the-art technique, MOSA, for testing REST APIs. Our evaluation shows promising results for using MISH to guide the automated test case generation within EvoMaster&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DRVN at the ICST 2025 Tool Competition – Self-Driving Car Testing Track</title>
      <link>https://example.com/publication/icst2025-competition/</link>
      <pubDate>Wed, 01 Jan 2025 00:03:02 +0100</pubDate>
      <guid>https://example.com/publication/icst2025-competition/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;DRVN is a regression testing tool that aims to diversify the test scenarios (road maps) to execute for testing and validating self-driving cars. DRVN harnesses the power of convolutional neural networks to identify possible failing roads in a set of generated examples before applying a greedy algorithm that selects and prioritizes the most diverse roads during regression testing. Initial testing discovered that DRVN performed well against random-based test selection.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rocket: A System-Level Fuzz-Testing Framework for the XRPL Consensus Algorithm</title>
      <link>https://example.com/publication/icst2025-tool/</link>
      <pubDate>Wed, 01 Jan 2025 00:03:02 +0100</pubDate>
      <guid>https://example.com/publication/icst2025-tool/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Byzantine fault tolerant algorithms are critical for achieving consistency and reliability in distributed systems, especially in the presence of faults or adversarial behavior. The consensus algorithm used by the XRP Ledger falls into this category. In practice, the implementation of these algorithms is prone to errors, which can lead to undesired behavior in the system. This paper introduces Rocket, a fuzz-testing framework designed for the XRPL consensus algorithm. Rocket enables researchers and developers to automatically inject network and process faults into a locally simulated network of XRPL validator nodes to test if the system behaves as expected. This technique has previously been shown to be effective in finding implementation errors. Rocket has been designed to focus on extensibility and ease of use, enabling users to run complex test scenarios with minimal setup. Video: &lt;a href=&#34;https://www.youtube.com/watch?v=07Z3ufRa51Y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=07Z3ufRa51Y&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Higher Fault Detection Through Novel Density Estimators in Unit Test Generation</title>
      <link>https://example.com/publication/ssbse2024/</link>
      <pubDate>Thu, 06 Jun 2024 09:16:23 +0200</pubDate>
      <guid>https://example.com/publication/ssbse2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CATMA: Conformance Analysis Tool For Microservice Applications</title>
      <link>https://example.com/publication/icse2024-tool/</link>
      <pubDate>Mon, 01 Jan 2024 19:37:11 +0200</pubDate>
      <guid>https://example.com/publication/icse2024-tool/</guid>
      <description>&lt;p&gt;Abstract:&lt;/p&gt;
&lt;p&gt;The microservice architecture allows developers to divide the core functionality of their software system into multiple smaller services. However, this architectural style also makes it harder for them to debug and assess whether the system&amp;rsquo;s deployment conforms to its implementation. We present CATMA, an automated tool that detects non-conformances between the system&amp;rsquo;s deployment and implementation. It automatically visualizes and generates potential interpretations for the detected discrepancies. Our evaluation of CATMA shows promising results in terms of performance and providing useful insights. CATMA is available at &lt;a href=&#34;https://cyber-analytics.nl/catma.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cyber-analytics.nl/catma.github.io/&lt;/a&gt;, and a demonstration video is available at &lt;a href=&#34;https://youtu.be/WKP1hG-TDKc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://youtu.be/WKP1hG-TDKc&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evolutionary Approach for Concurrency Testing of Ripple Blockchain Consensus Algorithm</title>
      <link>https://example.com/publication/icse-seip2023/</link>
      <pubDate>Sun, 01 Jan 2023 22:27:51 +0200</pubDate>
      <guid>https://example.com/publication/icse-seip2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generating Class-Level Integration Tests Using Call Site Information</title>
      <link>https://example.com/publication/ieee-tse2022/</link>
      <pubDate>Tue, 13 Sep 2022 09:29:14 +0200</pubDate>
      <guid>https://example.com/publication/ieee-tse2022/</guid>
      <description>&lt;p&gt;Abstract: Search-based approaches have been used in the literature to automate the process of creating unit test cases. However, related work has shown that generated tests with high code coverage could be ineffective, i.e., they may not detect all faults or kill all injected mutants. In this paper, we propose CLING, an integration-level test case generation approach that exploits how a pair of classes, the caller and the callee, interact with each other through method calls. In particular, CLING generates integration-level test cases that maximize the Coupled Branches Criterion (CBC). Coupled branches are pairs of branches containing a branch of the caller and a branch of the callee such that an integration test that exercises the former also exercises the latter. CBC is a novel integration-level coverage criterion, measuring the degree to which a test suite exercises the interactions between a caller and its callee classes. We implemented CLING and evaluated the approach on 140 pairs of classes from five different open-source Java projects. Our results show that (1) CLING generates test suites with high CBC coverage, thanks to the definition of the test suite generation as a many-objectives problem where each couple of branches is an independent objective; (2) such generated suites trigger different class interactions and can kill on average 7.7% (with a maximum of 50%) of mutants that are not detected by tests generated randomly or at the unit level; (3) CLING can detect integration faults coming from wrong assumptions about the usage of the callee class (25 for our subject systems) that remain undetected when using automatically generated random and unit-level test suites.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Guess What: Test Case Generation for Javascript with Unsupervised Probabilistic Type Inference</title>
      <link>https://example.com/publication/ssbse2022/</link>
      <pubDate>Thu, 18 Aug 2022 09:16:23 +0200</pubDate>
      <guid>https://example.com/publication/ssbse2022/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Search-based test case generation approaches make use of static type information to determine which data types should be used for the creation of new test cases. Dynamically typed languages like JavaScript, however, do not have this type information. In this paper, we propose an unsupervised probabilistic type inference approach to infer data types within the test case generation process. We evaluated the proposed approach on a benchmark of 98~units under test (i.e., exported classes and functions) compared to random type sampling w.r.t. branch coverage. Our results show that our type inference approach achieves a statistically significant increase in 56% of the test files with up to 71% of branch coverage compared to the baseline.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Log-based Slicing for System-level Test Cases</title>
      <link>https://example.com/publication/issta2021/</link>
      <pubDate>Mon, 19 Apr 2021 09:03:56 +0200</pubDate>
      <guid>https://example.com/publication/issta2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What Are We Really Testing in Mutation Testing for Machine Learning? A Critical Reflection</title>
      <link>https://example.com/publication/icse-nier2020/</link>
      <pubDate>Mon, 18 Jan 2021 14:29:50 +0200</pubDate>
      <guid>https://example.com/publication/icse-nier2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Kill Them All: An Exploratory Study on the Impact of Code Observability on Mutation Testing</title>
      <link>https://example.com/publication/jss2020/</link>
      <pubDate>Tue, 01 Dec 2020 17:32:54 +0200</pubDate>
      <guid>https://example.com/publication/jss2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Serverless Testing: Tool Vendors&#39; and Experts&#39; Point of View</title>
      <link>https://example.com/publication/ieee-software2020/</link>
      <pubDate>Wed, 07 Oct 2020 10:14:08 +0200</pubDate>
      <guid>https://example.com/publication/ieee-software2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Crash Reproduction Using Helper Objectives</title>
      <link>https://example.com/publication/gecco2020/</link>
      <pubDate>Sat, 04 Jul 2020 18:56:54 +0200</pubDate>
      <guid>https://example.com/publication/gecco2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automated Test Generation for Unit Testing Beyond</title>
      <link>https://example.com/talk/automated-test-generation-for-unit-testing-beyond/</link>
      <pubDate>Mon, 28 Oct 2019 11:17:15 +0200</pubDate>
      <guid>https://example.com/talk/automated-test-generation-for-unit-testing-beyond/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Testing Autonomous Cars for Feature Interaction Failures using Evolutionary Intelligence</title>
      <link>https://example.com/talk/testing-autonomous-cars-for-feature-interaction-failures-using-evolutionary-intelligence/</link>
      <pubDate>Tue, 01 Oct 2019 11:17:15 +0200</pubDate>
      <guid>https://example.com/talk/testing-autonomous-cars-for-feature-interaction-failures-using-evolutionary-intelligence/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effective and Efficient API Misuse Detection via Exception Propagation and Search-based Testing</title>
      <link>https://example.com/publication/issta2019/</link>
      <pubDate>Wed, 01 May 2019 20:22:51 +0200</pubDate>
      <guid>https://example.com/publication/issta2019/</guid>
      <description>&lt;p&gt;&lt;b&gt; Abstract &lt;/b&gt;: Application Programming Interfaces (APIs)
typically come with (implicit) usage constraints.
The violations of these constraints (API misuses)
can lead to software crashes.
Even though there are several tools that
can detect API misuses,
most of them suffer from a very high rate of false positives.
We introduce Catcher, a novel API misuse detection approach
that combines static exception propagation analysis with automatic search-based test case
generation to effectively and efficiently pinpoint crash-prone API misuses
in client applications.
We validate Catcher against 21 Java applications,
targeting misuses of the Java platform&amp;rsquo;s API.
Our results indicate that Catcher is able to generate
test cases that uncover 243 (unique) API misuses that result in
crashes.
Our empirical evaluation shows that Catcher can detect a large number of misuses (77 cases)
that would remain undetected by the traditional coverage-based test case generator EvoSuite.
Additionally, Catcher is on average eight times faster than EvoSuite
in generating test cases for the identified misuses.
Finally, we find that the majority of the exceptions triggered by Catcher
are unexpected to developers i.e., not only unhandled in the source code but also not listed in the documentation of the client applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Search-Based Test Data Generation for SQL Queries</title>
      <link>https://example.com/publication/icse2018/</link>
      <pubDate>Fri, 20 Jul 2018 00:04:04 +0200</pubDate>
      <guid>https://example.com/publication/icse2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Developer Testing in The IDE: Patterns, Beliefs, And Behavior</title>
      <link>https://example.com/publication/ieee-tse2018c/</link>
      <pubDate>Tue, 10 Jul 2018 11:24:46 +0200</pubDate>
      <guid>https://example.com/publication/ieee-tse2018c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Catch &#39;Em All: WatchDog, a Family of IDE Plug-Ins to Assess Testing</title>
      <link>https://example.com/publication/serip2016/</link>
      <pubDate>Thu, 18 Aug 2016 07:54:15 +0200</pubDate>
      <guid>https://example.com/publication/serip2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The impact of test case summaries on bug fixing performance: An empirical investigation</title>
      <link>https://example.com/publication/icse2016/</link>
      <pubDate>Thu, 18 Aug 2016 07:43:26 +0200</pubDate>
      <guid>https://example.com/publication/icse2016/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>

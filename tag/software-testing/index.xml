<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>software testing | Annibale Panichella</title>
    <link>https://example.com/tag/software-testing/</link>
      <atom:link href="https://example.com/tag/software-testing/index.xml" rel="self" type="application/rss+xml" />
    <description>software testing</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 06 Jun 2024 09:16:23 +0200</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>software testing</title>
      <link>https://example.com/tag/software-testing/</link>
    </image>
    
    <item>
      <title>Higher Fault Detection Through Novel Density Estimators in Unit Test Generation</title>
      <link>https://example.com/publication/ssbse2024/</link>
      <pubDate>Thu, 06 Jun 2024 09:16:23 +0200</pubDate>
      <guid>https://example.com/publication/ssbse2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CATMA: Conformance Analysis Tool For Microservice Applications</title>
      <link>https://example.com/publication/icse2024-tool/</link>
      <pubDate>Mon, 01 Jan 2024 19:37:11 +0200</pubDate>
      <guid>https://example.com/publication/icse2024-tool/</guid>
      <description>&lt;p&gt;Abstract:&lt;/p&gt;
&lt;p&gt;The microservice architecture allows developers to divide the core functionality of their software system into multiple smaller services. However, this architectural style also makes it harder for them to debug and assess whether the system&amp;rsquo;s deployment conforms to its implementation. We present CATMA, an automated tool that detects non-conformances between the system&amp;rsquo;s deployment and implementation. It automatically visualizes and generates potential interpretations for the detected discrepancies. Our evaluation of CATMA shows promising results in terms of performance and providing useful insights. CATMA is available at &lt;a href=&#34;https://cyber-analytics.nl/catma.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cyber-analytics.nl/catma.github.io/&lt;/a&gt;, and a demonstration video is available at &lt;a href=&#34;https://youtu.be/WKP1hG-TDKc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://youtu.be/WKP1hG-TDKc&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evolutionary Approach for Concurrency Testing of Ripple Blockchain Consensus Algorithm</title>
      <link>https://example.com/publication/icse-seip2023/</link>
      <pubDate>Sun, 01 Jan 2023 22:27:51 +0200</pubDate>
      <guid>https://example.com/publication/icse-seip2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generating Class-Level Integration Tests Using Call Site Information</title>
      <link>https://example.com/publication/ieee-tse2022/</link>
      <pubDate>Tue, 13 Sep 2022 09:29:14 +0200</pubDate>
      <guid>https://example.com/publication/ieee-tse2022/</guid>
      <description>&lt;p&gt;Abstract: Search-based approaches have been used in the literature to automate the process of creating unit test cases. However, related work has shown that generated tests with high code coverage could be ineffective, i.e., they may not detect all faults or kill all injected mutants. In this paper, we propose CLING, an integration-level test case generation approach that exploits how a pair of classes, the caller and the callee, interact with each other through method calls. In particular, CLING generates integration-level test cases that maximize the Coupled Branches Criterion (CBC). Coupled branches are pairs of branches containing a branch of the caller and a branch of the callee such that an integration test that exercises the former also exercises the latter. CBC is a novel integration-level coverage criterion, measuring the degree to which a test suite exercises the interactions between a caller and its callee classes. We implemented CLING and evaluated the approach on 140 pairs of classes from five different open-source Java projects. Our results show that (1) CLING generates test suites with high CBC coverage, thanks to the definition of the test suite generation as a many-objectives problem where each couple of branches is an independent objective; (2) such generated suites trigger different class interactions and can kill on average 7.7% (with a maximum of 50%) of mutants that are not detected by tests generated randomly or at the unit level; (3) CLING can detect integration faults coming from wrong assumptions about the usage of the callee class (25 for our subject systems) that remain undetected when using automatically generated random and unit-level test suites.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Guess What: Test Case Generation for Javascript with Unsupervised Probabilistic Type Inference</title>
      <link>https://example.com/publication/ssbse2022/</link>
      <pubDate>Thu, 18 Aug 2022 09:16:23 +0200</pubDate>
      <guid>https://example.com/publication/ssbse2022/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Search-based test case generation approaches make use of static type information to determine which data types should be used for the creation of new test cases. Dynamically typed languages like JavaScript, however, do not have this type information. In this paper, we propose an unsupervised probabilistic type inference approach to infer data types within the test case generation process. We evaluated the proposed approach on a benchmark of 98~units under test (i.e., exported classes and functions) compared to random type sampling w.r.t. branch coverage. Our results show that our type inference approach achieves a statistically significant increase in 56% of the test files with up to 71% of branch coverage compared to the baseline.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Log-based Slicing for System-level Test Cases</title>
      <link>https://example.com/publication/issta2021/</link>
      <pubDate>Mon, 19 Apr 2021 09:03:56 +0200</pubDate>
      <guid>https://example.com/publication/issta2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What Are We Really Testing in Mutation Testing for Machine Learning? A Critical Reflection</title>
      <link>https://example.com/publication/icse-nier2020/</link>
      <pubDate>Mon, 18 Jan 2021 14:29:50 +0200</pubDate>
      <guid>https://example.com/publication/icse-nier2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Kill Them All: An Exploratory Study on the Impact of Code Observability on Mutation Testing</title>
      <link>https://example.com/publication/jss2020/</link>
      <pubDate>Tue, 01 Dec 2020 17:32:54 +0200</pubDate>
      <guid>https://example.com/publication/jss2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Serverless Testing: Tool Vendors&#39; and Experts&#39; Point of View</title>
      <link>https://example.com/publication/ieee-software2020/</link>
      <pubDate>Wed, 07 Oct 2020 10:14:08 +0200</pubDate>
      <guid>https://example.com/publication/ieee-software2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Crash Reproduction Using Helper Objectives</title>
      <link>https://example.com/publication/gecco2020/</link>
      <pubDate>Sat, 04 Jul 2020 18:56:54 +0200</pubDate>
      <guid>https://example.com/publication/gecco2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automated Test Generation for Unit Testing Beyond</title>
      <link>https://example.com/talk/automated-test-generation-for-unit-testing-beyond/</link>
      <pubDate>Mon, 28 Oct 2019 11:17:15 +0200</pubDate>
      <guid>https://example.com/talk/automated-test-generation-for-unit-testing-beyond/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Testing Autonomous Cars for Feature Interaction Failures using Evolutionary Intelligence</title>
      <link>https://example.com/talk/testing-autonomous-cars-for-feature-interaction-failures-using-evolutionary-intelligence/</link>
      <pubDate>Tue, 01 Oct 2019 11:17:15 +0200</pubDate>
      <guid>https://example.com/talk/testing-autonomous-cars-for-feature-interaction-failures-using-evolutionary-intelligence/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effective and Efficient API Misuse Detection via Exception Propagation and Search-based Testing</title>
      <link>https://example.com/publication/issta2019/</link>
      <pubDate>Wed, 01 May 2019 20:22:51 +0200</pubDate>
      <guid>https://example.com/publication/issta2019/</guid>
      <description>&lt;p&gt;&lt;b&gt; Abstract &lt;/b&gt;: Application Programming Interfaces (APIs)
typically come with (implicit) usage constraints.
The violations of these constraints (API misuses)
can lead to software crashes.
Even though there are several tools that
can detect API misuses,
most of them suffer from a very high rate of false positives.
We introduce Catcher, a novel API misuse detection approach
that combines static exception propagation analysis with automatic search-based test case
generation to effectively and efficiently pinpoint crash-prone API misuses
in client applications.
We validate Catcher against 21 Java applications,
targeting misuses of the Java platform&amp;rsquo;s API.
Our results indicate that Catcher is able to generate
test cases that uncover 243 (unique) API misuses that result in
crashes.
Our empirical evaluation shows that Catcher can detect a large number of misuses (77 cases)
that would remain undetected by the traditional coverage-based test case generator EvoSuite.
Additionally, Catcher is on average eight times faster than EvoSuite
in generating test cases for the identified misuses.
Finally, we find that the majority of the exceptions triggered by Catcher
are unexpected to developers i.e., not only unhandled in the source code but also not listed in the documentation of the client applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Search-Based Test Data Generation for SQL Queries</title>
      <link>https://example.com/publication/icse2018/</link>
      <pubDate>Fri, 20 Jul 2018 00:04:04 +0200</pubDate>
      <guid>https://example.com/publication/icse2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Developer Testing in The IDE: Patterns, Beliefs, And Behavior</title>
      <link>https://example.com/publication/ieee-tse2018c/</link>
      <pubDate>Tue, 10 Jul 2018 11:24:46 +0200</pubDate>
      <guid>https://example.com/publication/ieee-tse2018c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Catch &#39;Em All: WatchDog, a Family of IDE Plug-Ins to Assess Testing</title>
      <link>https://example.com/publication/serip2016/</link>
      <pubDate>Thu, 18 Aug 2016 07:54:15 +0200</pubDate>
      <guid>https://example.com/publication/serip2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The impact of test case summaries on bug fixing performance: An empirical investigation</title>
      <link>https://example.com/publication/icse2016/</link>
      <pubDate>Thu, 18 Aug 2016 07:43:26 +0200</pubDate>
      <guid>https://example.com/publication/icse2016/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
